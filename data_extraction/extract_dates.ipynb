{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each source below, create a file containing all date-related metadata for each record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# SHARE data file, each line is a record\n",
    "SHARE_FILE = os.path.join('..', '..', 'data', 'share-jan-2019.json')\n",
    "SHARE_OUTPUT_FILE = os.path.join('..', '..', 'data', 'share_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fields = ['date', \n",
    "               'date_created',\n",
    "               'date_modified',\n",
    "               'date_updated',\n",
    "               'date_published'\n",
    "]\n",
    "\n",
    "fields_to_extract = ['osf_id', 'arxiv_id', 'doi'] + date_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "\n",
    "with open(SHARE_OUTPUT_FILE, 'w') as o:\n",
    "    writer = csv.DictWriter(o, fieldnames = fields_to_extract)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    with open(SHARE_FILE) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            osf_ids = set()\n",
    "            arxiv_ids = set()\n",
    "            dois = set()\n",
    "            for identifier in data['identifiers']:\n",
    "                if identifier.startswith('http://osf.io/'):\n",
    "                    osf_id = identifier.replace('http://osf.io/', '')\n",
    "                    if osf_id.endswith('/'):\n",
    "                        osf_id = osf_id[:-1]\n",
    "                        \n",
    "                    if '|' in osf_id:\n",
    "                        raise Exception('Pipe character used in identifier. Change delimiter')\n",
    "                    osf_ids.add(osf_id)\n",
    "                \n",
    "                elif identifier.startswith('http://arxiv.org/abs/'):\n",
    "                    arxiv_id = identifier.replace('http://arxiv.org/abs/', '')\n",
    "                    if arxiv_id.endswith('/'):\n",
    "                        arxiv_id = arxiv_id[:-1]\n",
    "                    if arxiv_id[-2] == 'v' and arxiv_id[-1].isdigit():\n",
    "                        arxiv_id = arxiv_id[:-2]\n",
    "                    if arxiv_id[-3] == 'v' and arxiv_id[-2].isdigit() and arxiv_id[-1].isdigit():\n",
    "                        arxiv_id = arxiv_id[:-3]\n",
    "                        \n",
    "                    if '|' in arxiv_id:\n",
    "                        raise ValueError('Pipe character used in identifier. Change delimiter')\n",
    "                    arxiv_ids.add(arxiv_id)\n",
    "            \n",
    "                elif identifier.startswith('http://dx.doi.org/'):\n",
    "                    doi = identifier.replace('http://dx.doi.org/', '')\n",
    "                    if doi.endswith('/'):\n",
    "                        doi = doi[:-1]\n",
    "                                             \n",
    "                    if '|' in doi:\n",
    "                        raise Exception('Pipe character used in identifier. Change delimiter')\n",
    "                    dois.add(doi)\n",
    "            \n",
    "            row = {}\n",
    "            row['osf_id'] = '|'.join(osf_ids)\n",
    "            row['arxiv_id'] = '|'.join(arxiv_ids)\n",
    "            row['doi'] = '|'.join(dois)\n",
    "                                             \n",
    "            for date_field in date_fields:\n",
    "                row[date_field] = data[date_field]     \n",
    "                \n",
    "            writer.writerow(row)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSF PREPRINT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OSF_PREPRINT_FILE = os.path.join('..', '..', 'raw_data', 'osf.json')\n",
    "OSF_PREPRINT_OUTPUT_FILE = os.path.join('..', '..', 'data', 'osf_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fields = ['date_last_transitioned', \n",
    "               'date_modified',\n",
    "               'original_publication_date',\n",
    "               'date_published',\n",
    "               'date_withdrawn', \n",
    "               'preprint_doi_created',\n",
    "               'date_created']\n",
    "\n",
    "fields_to_extract = ['id'] + date_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "\n",
    "with open(OSF_PREPRINT_OUTPUT_FILE, 'w') as o:\n",
    "    writer = csv.DictWriter(o, fieldnames = fields_to_extract)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    with open(OSF_PREPRINT_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            for record in data['data']:\n",
    "                row = {'id' : record['id'] }\n",
    "                \n",
    "                for date_field in date_fields:\n",
    "                    row[date_field] = record['attributes'][date_field]\n",
    "                \n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ARXIV_FOLDER = os.path.join('..', '..', 'raw_data', 'arXiv')\n",
    "ARXIVRAW_FOLDER = os.path.join('..', '..', 'raw_data', 'arXivRaw')\n",
    "\n",
    "ARXIV_OUTPUT_FILE = os.path.join('..', '..', 'data', 'arxiv_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET, csv\n",
    "\n",
    "arxiv_files = os.listdir(ARXIV_FOLDER)\n",
    "if '.DS_Store' in arxiv_files:\n",
    "    arxiv_files.remove('.DS_Store')\n",
    "    \n",
    "namespaces = {'oai' : 'http://www.openarchives.org/OAI/2.0/',\n",
    "              'arxiv' : 'http://arxiv.org/OAI/arXiv/' }\n",
    "\n",
    "arxiv_data = {}\n",
    "for arxiv_file in arxiv_files:\n",
    "    root = ET.parse(os.path.join(ARXIV_FOLDER, arxiv_file))\n",
    "\n",
    "    for record in root.findall('oai:ListRecords/oai:record', namespaces):\n",
    "        row = {}\n",
    "        row['id'] = record.find('oai:header/oai:identifier', namespaces).text.replace('oai:arXiv.org:', '')\n",
    "\n",
    "        row['datestamp'] = record.find('oai:header/oai:datestamp', namespaces).text\n",
    "\n",
    "        row['created'] = record.find('oai:metadata/arxiv:arXiv/arxiv:created', namespaces)\n",
    "        if row['created'] is not None:\n",
    "            row['created'] = row['created'].text\n",
    "\n",
    "        row['updated'] = record.find('oai:metadata/arxiv:arXiv/arxiv:updated', namespaces)\n",
    "        if row['updated'] is not None:\n",
    "            row['updated'] = row['updated'].text\n",
    "            \n",
    "        arxiv_data[row['id']] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxivraw_files = os.listdir(ARXIVRAW_FOLDER)\n",
    "if '.DS_Store' in arxivraw_files:\n",
    "    arxivraw_files.remove('.DS_Store')\n",
    "    \n",
    "namespaces = {'oai' : 'http://www.openarchives.org/OAI/2.0/',\n",
    "              'arxiv' : 'http://arxiv.org/OAI/arXivRaw/' }\n",
    "\n",
    "for arxivraw_file in arxivraw_files:\n",
    "    root = ET.parse(os.path.join(ARXIVRAW_FOLDER, arxivraw_file))\n",
    "\n",
    "    for record in root.findall('oai:ListRecords/oai:record', namespaces):\n",
    "        id_ = record.find('oai:header/oai:identifier', namespaces).text.replace('oai:arXiv.org:', '')\n",
    "        \n",
    "        if id_ in arxiv_data:\n",
    "            version_xml = record.findall('oai:metadata/arxiv:arXivRaw/arxiv:version', namespaces)\n",
    "            versions = {}\n",
    "            for version in version_xml:\n",
    "                versions[int(version.attrib['version'][1:])] = version.find('arxiv:date', namespaces).text\n",
    "\n",
    "            # concatenate all version dates, delimited by pipe character\n",
    "            version_str = ''\n",
    "            for i in range(len(versions.keys())):\n",
    "                version_str += versions[i + 1] + '|'\n",
    "            version_str = version_str[:-1]\n",
    "\n",
    "            arxiv_data[id_]['versions'] = version_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARXIV_OUTPUT_FILE, 'w') as o:\n",
    "    fieldnames = ['id', 'datestamp', 'created', 'updated', 'versions']\n",
    "    \n",
    "    writer = csv.DictWriter(o, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in arxiv_data.values():\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSSREF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CROSSREF_FILE = os.path.join('..', '..', 'raw_data', 'CrossRef.json')\n",
    "CROSSREF_OUTPUT_FILE = os.path.join('..', '..', 'data', 'crossref_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all date fields first\n",
    "\n",
    "import json\n",
    "\n",
    "crossref_ids = set() #using a set in case there are any duplicate ids\n",
    "\n",
    "date_keys = set()\n",
    "with open(CROSSREF_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        for key in data.keys():\n",
    "            if isinstance(data[key], dict) and 'date-parts' in data[key]:\n",
    "                date_keys.add(key)\n",
    "            elif isinstance(data[key], dict) and 'date-time' in data[key]:\n",
    "                date_keys.add(key)\n",
    "            elif isinstance(data[key], dict) and 'timestamp' in data[key]:\n",
    "                date_keys.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accepted', 'created', 'deposited', 'indexed', 'issued', 'posted'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fields = list(date_keys)\n",
    "\n",
    "fields_to_extract = ['id'] + date_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "\n",
    "with open(CROSSREF_OUTPUT_FILE, 'w') as o:\n",
    "    writer = csv.DictWriter(o, fieldnames = fields_to_extract)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    with open(CROSSREF_FILE, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            row = {'id' : data['DOI'] }\n",
    "                \n",
    "            for date_field in date_fields:\n",
    "                if date_field in data:\n",
    "                    if 'date-time' in data[date_field]:\n",
    "                        row[date_field] = data[date_field]['date-time']\n",
    "                    elif 'date-parts' in data[date_field]:\n",
    "                        # date-parts is a list but should only have one value\n",
    "                        if len(data[date_field]['date-parts']) > 1:\n",
    "                            raise ValueError('Multiple date-parts')\n",
    "                            \n",
    "                        row[date_field] = '-'.join([str(s) for s in data[date_field]['date-parts'][0]])\n",
    "                \n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
